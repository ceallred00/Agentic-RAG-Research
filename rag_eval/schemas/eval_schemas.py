"""Defines Pydantic models for the evaluation agent and the dataset."""

from pydantic import BaseModel, Field
from typing import Annotated, Literal, TypedDict, Sequence, List
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage
from schemas.agent_schemas import AgentMetadata

# TODO: Evaluate adding expected contexts (Header Hierachy, URL, etc.)

# TODO: Evaluate adding primary key for EvalDatasetRow for O(1) lookup during eval.

class EvalDatasetRow(BaseModel):
    """
    Represents a single evaluation sample with a question and its ground truth answer
    """
    question: Annotated[str, Field(min_length=1,description = "Natural language query to evaluate retrieval against.")]
    ground_truth: Annotated[str, Field(min_length=1, description = "Expected correct answer used by RAGAS to judge context recall.")]

class RetrievalResult(BaseModel):
    """
    Aggregates all Pinecone matches for a single evaluation query, including retrieved text, scores, and
    metadata
    """
    question: Annotated[str, Field(description = "Natural language query to evaluate retrieval against.")]
    contexts: Annotated[List[str], Field(description = "Per-match enriched contexts used by Pinecone to evaluate similarity score.")]
    metadata: Annotated[List[dict], Field(description = "Per-match metadata dicts containing source, headers, URL, and other fields stored during ingestion.")]
    scores: Annotated[List[float], Field(description = "List of similarity score returned by Pinecone for each match. Higher scores indicate higher similarity to the user's query")]
    ids: Annotated[List[str], Field(description = "List of vector IDs returned for traceability.")]

class QuestionEvalResult(BaseModel):
    """
    Represents the returned RAGAS metrics for a single evaluation query, as well as the retrieved contexts.
    """
    question: Annotated[str, Field(description = "Natural language query to evaluate retrieval against.")]
    context_precision: Annotated[float, Field(description = "Metric that evaluates the retriever's ability to rank relevant chunks higher than irrelevant chunks for a given query. The returned value should be between 0 and 1.")]
    context_recall: Annotated[float, Field(description = "Measures how many of the relevant pieces of information were successfully retrieved.")]
    contexts: Annotated[List[str], Field(description = "Per-match enriched contexts used by Pinecone to evaluate similarity score.")]

class EvalReport(BaseModel):
    """ Final aggregation of information."""
    average_context_recall: Annotated[float, Field(description = "Average context recall aggregated over the entire dataset")]
    average_context_precision: Annotated[float, Field(description = "Average context precision aggregated over the entire dataset")]
    total_questions_evaluated: Annotated[int, Field(description = "Number of questions evaluated.")]
    dataset_name: Annotated[str, Field(description = "Name of evaluated dataset.")]
    per_question_results: Annotated[List[QuestionEvalResult], Field(description = "Per-question results, including the question, context and precision scores, and the retrieved contexts.")]
    description: Annotated[str, Field(description = "Brief description of the evaluation run (e.g., 'Baseline hybrid search, top_k=5')")]


class EvalAgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]  # Accumulated messages with add_messages reducer
    loaded_dataset: List[EvalDatasetRow]  # Natural language queries and associated ground truth responses
    retrieval_results: List[RetrievalResult]  # Aggregated Pinecone matches for every question in the dataset
    eval_results: List[QuestionEvalResult]  # RAGAS metrics and retrieved contexts for each question
    final_report: EvalReport  # Aggregated performance report across the full dataset
    json_report_path: str  # Filepath to the saved JSON report
    md_report_path: str  # Filepath to the saved Markdown report
    llm_summary: str # Summary of evaluation results generated by the LLM

# TODO: Evaluate refactoring for overlap with AgentConfig, etc.

class EvalRagasLLMConfig(BaseModel):
    """
    Technical configuration for the RAGAS evaluation LLM.

    Uses a raw AsyncOpenAI client configured for Eden AI, wrapped via ragas.llms.llm_factory
    into an InstructorBaseRagasLLM. This is required because RAGAS metric scoring uses
    structured output generation internally, which is incompatible with the LangChain
    ChatOpenAI proxy used elsewhere.

    Both model_name and provider are required:
        - model_name: sent to Eden AI's API in provider/model format (e.g., 'openai/gpt-4o')
        - provider: used by llm_factory to select the correct internal adapter (e.g., 'openai'
          selects the Instructor adapter; 'google' selects the LiteLLM adapter)

    Temperature is set to 0.0 by default to ensure deterministic, reproducible metric
    scores across evaluation runs. Since RAGAS makes graded judgments about retrieval
    quality, consistency is more important than variation.
    """
    model_name: Annotated[str, Field(description="The model name in Eden AI's provider/model format (e.g., 'openai/gpt-4o').")]
    provider: Annotated[str, Field(description="The model provider passed to llm_factory for internal adapter selection (e.g., 'openai').")]
    temperature: Annotated[float, Field(default=0.0, ge=0.0, le=2.0, description="Sampling temperature for the RAGAS evaluation LLM. Defaults to 0.0 for deterministic scoring.")]

class EvalSummaryLLMConfig(BaseModel):
    """
    Technical configuration for the LLM used to generate a summary of evaluation results.

    Uses the LangChain ChatOpenAI proxy configured for Eden AI via ExecutionService.
    Only model_name is required since the client is constructed by ExecutionService,
    which handles authentication and base URL configuration internally.

    No provider field is needed here â€” the ChatOpenAI proxy accepts Eden AI's
    provider/model format directly as the model name (e.g., 'openai/gpt-4o-mini').
    """
    model_name: Annotated[str, Field(description="The model name in Eden AI's provider/model format (e.g., 'openai/gpt-4o-mini').")]

class EvalRetrieverConfig(BaseModel):
    """Configuration parameters for the retriever used in the evaluation."""
    type: Annotated[str, Field(description = "The type of retriever to use for the evaluation (e.g., 'StructuredRetriever').")]
    top_k_matches: Annotated[int, Field(description = "The number of top contexts to retrieve from Pinecone for each question.")]
    index_name: Annotated[str, Field(description = "The name of the Pinecone index to query for retrieval.")]
    # Only one current option for each embedding model due to compatibility requirements with model used to ingest the data.
    dense_embedding_model: Annotated[Literal["gemini-embedding-001"], Field(description = "The name of the dense embedding model to use for retrieval.")]
    sparse_embedding_model: Annotated[Literal["pinecone-sparse-english-v0"], Field(description = "The name of the sparse embedding model to use for retrieval.")]

class EvalReportConfig(BaseModel):
    """Configuration parameters for the evaluation report generation and saving."""
    output_dir: Annotated[str, Field(description = "The directory where evaluation reports should be saved.")]
    encoding: Annotated[str, Field(description = "The encoding format to use when saving evaluation reports (e.g., 'utf-8').")]

class EvalDataConfig(BaseModel):
    """Configuration parameters for the evaluation dataset loading and processing."""
    csv_dir: Annotated[str, Field(description = "The directory where CSV datasets are located relative to the 'rag_eval' folder.")]

class EvalAgentConfig(BaseModel):
    """Configuration parameters for the evaluation agent."""
    version: Annotated[str, Field(description = "Version of the evaluation agent configuration for compatibility checks.")]
    eval_agent_metadata: Annotated[AgentMetadata, Field(description = "Metadata about the evaluation agent.")]
    ragas_llm_model: Annotated[EvalRagasLLMConfig, Field(description = "Technical configuration for the RAGAS evaluation LLM.")]
    summary_llm_model: Annotated[EvalSummaryLLMConfig, Field(description = "Technical configuration for the LLM used to generate a summary of evaluation results.")]
    retriever: Annotated[EvalRetrieverConfig, Field(description = "Configuration parameters for the retriever used in the evaluation.")]
    report: Annotated[EvalReportConfig, Field(description = "Configuration parameters for the evaluation report generation and saving.")]
    data: Annotated[EvalDataConfig, Field(description = "Configuration parameters for the evaluation dataset loading and processing.")]





