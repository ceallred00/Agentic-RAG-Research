
from typing import List, Optional
from dotenv import load_dotenv
from langchain_core.messages import BaseMessage, ToolMessage, SystemMessage, AIMessage, HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.tools import tool
from langgraph.graph import StateGraph, END
from langgraph.graph.state import CompiledStateGraph
from langgraph.prebuilt import ToolNode
from langgraph.types import interrupt
from core.state import AgentState
from utils.application_streamer import application_streamer
from utils.process_events import process_events



load_dotenv()

@tool
def search_web(query: str):
    """
    Perform a web search for information related to the user's query. This tool call should follow a call to perform_rag, which will help in identifying relevant documents and resources.
    The web search should be performed only if the relevant results obtained from the perform_rag tool do not provide enough information to answer the user's query, and the user is not satisfied with the information provided.
    
    The user should be asked if they are satisfied with the information provided by the web search, using the determine_user_sentiment tool.
    If the user is not satisfied, ask the user if they would like to send an email to the advisor for further assistance.
    """
    return f"Web search results for query: {query}"  # Example response

@tool
def vectorize_user_input(user_input: str) -> List[float]:
    """ Vectorize user input for further processing.
    
    Unless a user explicitly requests to send an email, or to look up an advisor, this tool call should be the first step in generating a response. 
    The vectorized input should be used to perform RAG and generate a response, performed by the perform_rag tool.
    """
    return [-0.2, 0.1, 0.3, -0.1, 0.2]  # Example vectorized input

@tool
def perform_rag(user_query_vector: List[float]):
    """You have access to a large corpus of documents and resources related to the Univeristy of West Florida to answer the user's query. 
    Perform Retrieval-Augmented Generation (RAG) on the vectorized user input, generated by the vectorize_user_input tool. This tool call should follow a call to vectorize_user_input.
    The RAG process will help in generating a more accurate and relevant response to the user's query.
    The RAG process will also help in identifying relevant documents and resources that can be used to answer the user's query.

    
    Once the relevant results are obtained, return the results to the user. The results should include the most relevant documents and resources that can be used to answer the user's query.
    The results should also include a summary of the most relevant information found in the documents and resources.If the relevant results include a URL, include it in the response as well. 
    
    You should then ask the user if they are satisfied with the information provided, using the determine_user_sentiment tool.
    If the user is not satisfied, ask the user if they would like you to perform a web search for more information, or if they would like to send an email to the advisor for further assistance.
    If the user agrees, perform a web search using the search_web tool and return the results to the user.
    """
    return "Relevant results from RAG process"  # Example response 

@tool
def draft_email(user_input: str, advisor_email: str, advisor_name: str, student_name: str, student_email: str):
    """ Draft an email based on user input. This email will be used to communicate with the advisor. 
    The email should include the name of the student, the student's email address,the advisor's name, and a brief summary of the user's query. 
    The tone should be professional and courteous.
    
    This tool call should follow a call to search_for_advisor, which will help in identifying the advisor's email address and name.
    The email should be sent using the send_email tool.

    This tool call should be performed only if explicitly requested by the user, or if the user is not satisfied with the information provided by the RAG process or the web search.
    """
    #TODO: Implement the email response structure
    pass

@tool
def send_email(email_content: str):
    """ Send an email with the drafted content. This tool call should follow a call to draft_email.
    The email should be sent to the advisor's email address, as identified using the search_for_advisor tool."""

    pass

@tool
def search_for_advisor(department: str):
    """Search for an advisor based on the student's department. If the department is not specified, you should prompt the user for their department.
    If the department is specified, the tool will search for an advisor in that department.
    Once the advisor is found, return the email address of the advisor to the user.
    Then, ask the user if they would like to draft and send an email to the advisor for further assistance.
    If the user agrees, draft an email using the draft_email tool. 
    """
    fake_department_advisors = {
        "Computer Science": {"name": "Dr. Smith", "email": "jane.smith@uwf.edu"}, 
        "Mathematics": {"name": "Dr. Johnson", "email": "john.johnson@uwf.edu"},
        "Biology": {"name": "Dr. Lee", "email": "sarah.lee@uwf.edu"},
        "History": {"name": "Dr. Brown", "email": "michael.brown@uwf.edu"}
    }

    try:
        advisor_info = fake_department_advisors[department]
        advisor_name = advisor_info["name"]
        advisor_email = advisor_info["email"]
    except KeyError:
        return "I'm sorry, I couldn't find an advisor for that department. Please check the department name and try again."
    
    return f"Advisor found. Your advisor is {advisor_name} at {advisor_email}."  # Example response

@tool
def end_conversation() -> str:
    """ If the user is satisfied with the information provided, or if the user explicitly requests to end the conversation, this tool call should be used to end the conversation.
    Additional indications include "I'm done", "I'm satisfied", "I'm happy", "I'm good", "I'm fine", "I'm all set", "I'm all good", "I'm all set", "goodbye".
    """
    return "Conversation ended"  # Example response


# Register the available tools
tools = [search_web, vectorize_user_input, perform_rag, draft_email, send_email, search_for_advisor, end_conversation]

# Initialize the model with the available tools
model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview", include_thoughts=True).bind_tools(tools) # Bind available tools to the model

def base_agent(state: AgentState) -> AgentState:
    system_prompt = SystemMessage(content = f"""
        You are an AI assistant designed to help students with their academic and administrative tasks. 
        Your primary goal is to help answer students' questions and provide guidance on various topics. You may send an email to the advisor for further assistance if the user is not satisfied with the information provided.
        You only have access to information related to the University of West Florida.
        You have access to a variety of tools to assist with these tasks : {tools}

        Workflow:
            1. Greet the student with a friendly message, explaining who you are and what services you can provide.
        The user will provide input, and you will use the available tools to generate a response. 
        Unless the user has explicitly requested to send an email, or to look up an advisor, the first step in generating a response should be to vectorize the user's input using the vectorize_user_input tool.
        Once the user's input is vectorized, you should perform retrieval augmented generation (RAG) on the vectorized input using the perform_rag tool.
        If the user is not satisfied with the information provided by the RAG process, you should ask the user if they would like you to perform a web search for more information, or if they would like to send an email to the advisor for further assistance.
        If the user agrees to perform a web search, use the search_web tool to perform the search and return the results to the user.
        If the user agrees to send an email, first search for the advisor using the search_for_advisor tool. 
        Once the advisor is found, draft an email using the draft_email tool and return it to the user for review.
        Once the user is satisfied, send the email using the send_email tool.

        If the user is satisfied with the information provided, or if the user explicitly requests to end the conversation, first ask the user if they have any additional questions. 
        If the user has no additional questions, use the end_conversation tool to end the conversation.

        If you do not have all of the required arguments for a tool call, you should prompt the user for the required arguments.
        """)

    # Rules first + conversation history + most recent user message
    messages = [system_prompt] + state["messages"]

    # Generate a response using the model
    response = model.invoke(messages) 
    
    # Update the state by returning the old messages + the new user message + the new AI response
    return {"messages": [response]}

def should_continue(state: AgentState):
    """ 
    Conditional function to determine if the graph should continue or end.
    """
    messages = state["messages"]
    last_message = messages[-1]
    if isinstance(last_message, AIMessage) and not last_message.tool_calls: # Look at last message to see if there are any tool calls. If not, end the graph.
        return "end" # Edge
    else: # If there are tool calls, continue the graph.
        return "continue" # Edge

    
def print_messages(messages):
    """ Print the message in a more readable format."""
    if not messages:
        return
    
    for message in messages[-3:]:
        if isinstance(message, ToolMessage):
            print(f"\n TOOL RESULT: {message.content}")

graph = StateGraph(AgentState)
graph.add_node("base_agent", base_agent)
graph.add_node("tool_node", ToolNode(tools))

graph.set_entry_point("base_agent")
graph.add_conditional_edges(
    source = "base_agent", 
    path = should_continue,
    path_map = {
        "continue": "tool_node",
        "end": END
    
    }
)
graph.add_edge("tool_node", "base_agent")

# Initialize Memory Checkpointer
memory = MemorySaver()

# Returns a CompiledStateGraph object
app = graph.compile(checkpointer=memory)


def run():
    """
    Run the base agent conversation loop.

    The graph is invoked when the user provides input.
    The graph will execute according to its architecture.
     
    The conversation continues until the user decides to exit, meaning that the graph may be invoked several times depending on the user's input.
    
    """
    print("\n The AI Assistant is ready to help you. Type 'exit' to end the conversation.")
    # A static thread_id simulates a persistent user session.
    config = {"configurable": {"thread_id": "123"}}

    # current_state is a StateSnapshot object
    current_state = app.get_state(config)

    # current_state.values is a dictionary.
    if not current_state.values or not current_state.values.get("messages"):
        # Kickstart AI
        initial_input = "Hi there!"

        events = application_streamer(application=app,
                                      user_input = initial_input, 
                                      configuration = config, 
                                      stream_mode = "updates")
        
        process_events(events = events, thinking_flag = False)

    while True:
        try: 
            user_input = input("\nUser: ")
            if user_input.lower() in ["exit", "quit"]:
                print("Exiting the conversation.")
                break
        except (KeyboardInterrupt, EOFError):
            print("\nExiting the conversation.")
            break

        # Runs the graph one node at a time, streaming intermediate state.
        events = application_streamer(
            application = app, 
            user_input = user_input, 
            configuration = config, 
            stream_mode = "updates"
        )

        process_events(events = events, thinking_flag = True)


                    
    
if __name__ == "__main__":
    run()
